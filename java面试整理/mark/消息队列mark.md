1.为什么要用消息队列？

```
优点：
	-解耦
	-异步
	-应对高峰突发流量，削峰
代价：
	-系统复杂性变高
	-可用性降低
	-一致性问题
```

2.消息队列技术选型

```
kafka、activemq、rabbitmq、rocketmq

kafka：
	-10万级的单机吞吐量
	-延迟ms以内
	-可用性高
	-经过参数优化，消息0丢失
	-大数据界业内标准

rocketmq：
	-阿里系
	-10万级
	-ms级
	-分布式，高可用
	
rabbitmq
	-万级
	-微秒级
	-erlang开发
```

3.消息队列高可用

```
rabbitmq
	1.普通集群模式（queue数据只存储在单节点，其他存储元数据，可能会导致内部大量传输，宕机易造成数据丢失）
	2.镜像集群模式（每个节点，都存储元数据和数据，非分布式，性能受到单机制约）

kafka
	1.多个broker组成，每个broker是一个节点，创建一个topic，可以划分多个partition，每个partition可以存在于不同的broker上，每个partition存放数据。天然的分布式。
	2.HA机制，每个partition会同步到其他机器上，形成自己的多个副本，所有replica会选举出一个leader，剩余的就是follower，leader会负责和生产和消费打交道，并且同步数据到follower。
```

4.消息队列消费到重复数据

```
kafka
	1.kafka有个offset的概念，每个消息写进去的时候，会有个offset，代表消息序号，然后消费者消费了数据之后，每隔一定时间，会把自己消费过的offset消息提交一下。（zk记录）
	2.发生意外。需要业务方自己保证：
		-写库，写之前查一下。或者利用数据库唯一键约束。
		-redis，set，天然幂等。
		-或者给消息加个全局唯一id，然后处理之前查一下。
```

5.消息队列的可靠性传输

```
rabbitmq
	-生产者弄丢数据
		选择用rabbitmq提供的事务机制。消息没有被rabbitmq接受到，生产者会异常报错，可以 回滚事务，或者重发，如果接受到消息，则提交事务。但是这么做，并发量会大大降低。
		可以开启confirm机制，每个消息会分配唯一id，消息写入rabbitmq后，rabbitmq会回传一个ack消息。如果rabbitmq没能处理这个消息，会回调一个nack接口，可以处理。而且结合唯一id，一定时间没收到ack，可以重发。
		
   -rabbitmq弄丢数据
   	开启数据持久化，写入磁盘。持久化配合confirm机制。
   	
   -消费者弄丢数据
   	使用rabbitmq的ack机制，关闭自动ack，通过一个api调用，每次确认处理完，再ack。
```

```
kafka
1.消费者丢数据
	kafka会自动提交offset，关闭自动提交，再处理完后手动提交。但是自己需要保证幂等性。
	
2.kafaka丢数据
	配置4个参数
		-给topic设置必须至少有2个副本
		-要求每个leader至少感知一个follower正常联系
		-设置要求每条数据写入所有replic后，才返回写入成功
		-生产端设置一个很大的重试值
		
3.生产者丢数据
	不会丢数据
```

6.消息队列的顺序执行性

```
rabbitmq错乱场景：一个queue，多个消费者

解决方案：拆分多个queue，每个queue一个消费者，或者一个consumer hash给多个queue。
```

```
kafka错乱场景：一个topic，一个partition，一个consumer，内部多个线程

解决方案：内部单线程消费，写n个内存queue，n个线程每个消费一个queue
```

7.消息积压

```
-临时紧急扩容
  1.修复consumer，确保正常消费
  2.新建topic，partition是原来十倍，
  3.写临时分发程序，消费积压数据，临时写入新topic
  4.征用10倍临时机器，部署consumer消费。
```

8.设计一个消费队列中间件

```
1.支持伸缩性
2.考虑持久化
3.可用性
4.数据可靠性传输
```

